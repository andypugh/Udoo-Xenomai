***************
*** 845,850 ****
  	return pfn_to_page(pfn);
  }
  
  /*
   * copy one vm_area from one task to the other. Assumes the page tables
   * already present in the new task to be cleared in the whole range
--- 845,876 ----
  	return pfn_to_page(pfn);
  }
  
+ static inline void cow_user_page(struct page *dst, struct page *src, unsigned long va, struct vm_area_struct *vma)
+ {
+ 	/*
+ 	 * If the source page was a PFN mapping, we don't have
+ 	 * a "struct page" for it. We do a best-effort copy by
+ 	 * just copying from the original user address. If that
+ 	 * fails, we just zero-fill it. Live with it.
+ 	 */
+ 	if (unlikely(!src)) {
+ 		void *kaddr = kmap_atomic(dst, KM_USER0);
+ 		void __user *uaddr = (void __user *)(va & PAGE_MASK);
+ 
+ 		/*
+ 		 * This really shouldn't fail, because the page is there
+ 		 * in the page tables. But it might just be unreadable,
+ 		 * in which case we just give up and fill the result with
+ 		 * zeroes.
+ 		 */
+ 		if (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE))
+ 			clear_page(kaddr);
+ 		kunmap_atomic(kaddr, KM_USER0);
+ 		flush_dcache_page(dst);
+ 	} else
+ 		copy_user_highpage(dst, src, va, vma);
+ }
+ 
  /*
   * copy one vm_area from one task to the other. Assumes the page tables
   * already present in the new task to be cleared in the whole range
***************
*** 853,860 ****
  
  static inline unsigned long
  copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
- 		pte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *vma,
- 		unsigned long addr, int *rss)
  {
  	unsigned long vm_flags = vma->vm_flags;
  	pte_t pte = *src_pte;
--- 879,886 ----
  
  static inline unsigned long
  copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+ 	     pte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *vma,
+ 	     unsigned long addr, int *rss, struct page *uncow_page)
  {
  	unsigned long vm_flags = vma->vm_flags;
  	pte_t pte = *src_pte;
***************
*** 897,902 ****
  	 * in the parent and the child
  	 */
  	if (is_cow_mapping(vm_flags)) {
  		ptep_set_wrprotect(src_mm, addr, src_pte);
  		pte = pte_wrprotect(pte);
  	}
--- 923,943 ----
  	 * in the parent and the child
  	 */
  	if (is_cow_mapping(vm_flags)) {
+ #ifdef CONFIG_IPIPE
+ 		if (uncow_page) {
+ 			struct page *old_page = vm_normal_page(vma, addr, pte);
+ 			cow_user_page(uncow_page, old_page, addr, vma);
+ 			pte = mk_pte(uncow_page, vma->vm_page_prot);
+ 
+ 			if (vm_flags & VM_SHARED)
+ 				pte = pte_mkclean(pte);
+ 			pte = pte_mkold(pte);
+ 
+ 			page_add_new_anon_rmap(uncow_page, vma, addr);
+ 			rss[!!PageAnon(uncow_page)]++;
+ 			goto out_set_pte;
+ 		}
+ #endif /* CONFIG_IPIPE */
  		ptep_set_wrprotect(src_mm, addr, src_pte);
  		pte = pte_wrprotect(pte);
  	}
***************
*** 934,946 ****
  	int progress = 0;
  	int rss[NR_MM_COUNTERS];
  	swp_entry_t entry = (swp_entry_t){0};
- 
  again:
  	init_rss_vec(rss);
  
  	dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
- 	if (!dst_pte)
  		return -ENOMEM;
  	src_pte = pte_offset_map(src_pmd, addr);
  	src_ptl = pte_lockptr(src_mm, src_pmd);
  	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
--- 975,1001 ----
  	int progress = 0;
  	int rss[NR_MM_COUNTERS];
  	swp_entry_t entry = (swp_entry_t){0};
+ 	struct page *uncow_page = NULL;
+ #ifdef CONFIG_IPIPE
+ 	int do_cow_break = 0;
  again:
+  	if (do_cow_break) {
+  		uncow_page = alloc_page_vma(GFP_HIGHUSER, vma, addr);
+ 		if (uncow_page == NULL)
+  			return -ENOMEM;
+ 		do_cow_break = 0;
+ 	}
+ #else
+ again:
+ #endif
  	init_rss_vec(rss);
  
  	dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
+ 	if (!dst_pte) {
+ 		if (uncow_page)
+ 			page_cache_release(uncow_page);
  		return -ENOMEM;
+ 	}
  	src_pte = pte_offset_map(src_pmd, addr);
  	src_ptl = pte_lockptr(src_mm, src_pmd);
  	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
***************
*** 963,970 ****
  			progress++;
  			continue;
  		}
  		entry.val = copy_one_pte(dst_mm, src_mm, dst_pte, src_pte,
- 							vma, addr, rss);
  		if (entry.val)
  			break;
  		progress += 8;
--- 1018,1042 ----
  			progress++;
  			continue;
  		}
+ #ifdef CONFIG_IPIPE
+ 		if (likely(uncow_page == NULL) && likely(pte_present(*src_pte))) {
+ 			if (is_cow_mapping(vma->vm_flags) &&
+ 			    test_bit(MMF_VM_PINNED, &src_mm->flags) &&
+ 			    ((vma->vm_flags|src_mm->def_flags) & VM_LOCKED)) {
+ 				arch_leave_lazy_mmu_mode();
+ 				spin_unlock(src_ptl);
+ 				pte_unmap(src_pte);
+ 				add_mm_rss_vec(dst_mm, rss);
+ 				pte_unmap_unlock(dst_pte, dst_ptl);
+ 				cond_resched();
+ 				do_cow_break = 1;
+ 				goto again;
+ 			}
+ 		}
+ #endif
  		entry.val = copy_one_pte(dst_mm, src_mm, dst_pte, src_pte,
+ 					 vma, addr, rss, uncow_page);
+ 		uncow_page = NULL;
  		if (entry.val)
  			break;
  		progress += 8;
***************
*** 1653,1659 ****
  
  	VM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));
  
- 	/* 
  	 * Require read or write permissions.
  	 * If FOLL_FORCE is set, we only require the "MAY" flags.
  	 */
--- 1725,1731 ----
  
  	VM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));
  
+ 	/*
  	 * Require read or write permissions.
  	 * If FOLL_FORCE is set, we only require the "MAY" flags.
  	 */
***************
*** 2438,2469 ****
  	return same;
  }
  
- static inline void cow_user_page(struct page *dst, struct page *src, unsigned long va, struct vm_area_struct *vma)
- {
- 	/*
- 	 * If the source page was a PFN mapping, we don't have
- 	 * a "struct page" for it. We do a best-effort copy by
- 	 * just copying from the original user address. If that
- 	 * fails, we just zero-fill it. Live with it.
- 	 */
- 	if (unlikely(!src)) {
- 		void *kaddr = kmap_atomic(dst, KM_USER0);
- 		void __user *uaddr = (void __user *)(va & PAGE_MASK);
- 
- 		/*
- 		 * This really shouldn't fail, because the page is there
- 		 * in the page tables. But it might just be unreadable,
- 		 * in which case we just give up and fill the result with
- 		 * zeroes.
- 		 */
- 		if (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE))
- 			clear_page(kaddr);
- 		kunmap_atomic(kaddr, KM_USER0);
- 		flush_dcache_page(dst);
- 	} else
- 		copy_user_highpage(dst, src, va, vma);
- }
- 
  /*
   * This routine handles present pages, when users try to write
   * to a shared page. It is done by copying the page to a new address
--- 2510,2515 ----
  	return same;
  }
  
  /*
   * This routine handles present pages, when users try to write
   * to a shared page. It is done by copying the page to a new address
***************
*** 3991,3993 ****
  	}
  }
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */
--- 4037,4155 ----
  	}
  }
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */
+ 
+ #ifdef CONFIG_IPIPE
+ 
+ static inline int ipipe_pin_pte_range(struct mm_struct *mm, pmd_t *pmd,
+ 				      struct vm_area_struct *vma,
+ 				      unsigned long addr, unsigned long end)
+ {
+ 	spinlock_t *ptl;
+ 	pte_t *pte;
+ 
+ 	do {
+ 		pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
+ 		if (!pte)
+ 			continue;
+ 
+ 		if (!pte_present(*pte) || pte_write(*pte)) {
+ 			pte_unmap_unlock(pte, ptl);
+ 			continue;
+ 		}
+ 
+ 		if (do_wp_page(mm, vma, addr, pte, pmd, ptl, *pte) == VM_FAULT_OOM)
+ 			return -ENOMEM;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 	return 0;
+ }
+ 
+ static inline int ipipe_pin_pmd_range(struct mm_struct *mm, pud_t *pud,
+ 				      struct vm_area_struct *vma,
+ 				      unsigned long addr, unsigned long end)
+ {
+ 	unsigned long next;
+ 	pmd_t *pmd;
+ 
+ 	pmd = pmd_offset(pud, addr);
+ 	do {
+ 		next = pmd_addr_end(addr, end);
+ 		if (pmd_none_or_clear_bad(pmd))
+ 			continue;
+ 		if (ipipe_pin_pte_range(mm, pmd, vma, addr, next))
+ 			return -ENOMEM;
+ 	} while (pmd++, addr = next, addr != end);
+ 	return 0;
+ }
+ 
+ static inline int ipipe_pin_pud_range(struct mm_struct *mm, pgd_t *pgd,
+ 				      struct vm_area_struct *vma,
+ 				      unsigned long addr, unsigned long end)
+ {
+ 	unsigned long next;
+ 	pud_t *pud;
+ 
+ 	pud = pud_offset(pgd, addr);
+ 	do {
+ 		next = pud_addr_end(addr, end);
+ 		if (pud_none_or_clear_bad(pud))
+ 			continue;
+ 		if (ipipe_pin_pmd_range(mm, pud, vma, addr, next))
+ 			return -ENOMEM;
+ 	} while (pud++, addr = next, addr != end);
+ 	return 0;
+ }
+ 
+ int __ipipe_pin_vma(struct mm_struct *mm, struct vm_area_struct *vma)
+ {
+ 	unsigned long addr, next, end;
+ 	pgd_t *pgd;
+ 
+ 	addr = vma->vm_start;
+ 	end = vma->vm_end;
+ 
+ 	pgd = pgd_offset(mm, addr);
+ 	do {
+ 		next = pgd_addr_end(addr, end);
+ 		if (pgd_none_or_clear_bad(pgd))
+ 			continue;
+ 		if (ipipe_pin_pud_range(mm, pgd, vma, addr, next))
+ 			return -ENOMEM;
+ 	} while (pgd++, addr = next, addr != end);
+ 
+ 	return 0;
+ }
+ 
+ int ipipe_disable_ondemand_mappings(struct task_struct *tsk)
+ {
+ 	struct vm_area_struct *vma;
+ 	struct mm_struct *mm;
+ 	int result = 0;
+ 
+ 	mm = get_task_mm(tsk);
+ 	if (!mm)
+ 		return -EPERM;
+ 
+ 	down_write(&mm->mmap_sem);
+ 	if (test_bit(MMF_VM_PINNED, &mm->flags))
+ 		goto done_mm;
+ 
+ 	for (vma = mm->mmap; vma; vma = vma->vm_next) {
+ 		if (!is_cow_mapping(vma->vm_flags)
+ 		    || !(vma->vm_flags & VM_WRITE))
+ 			continue;
+ 
+ 		result = __ipipe_pin_vma(mm, vma);
+ 		if (result < 0)
+ 			goto done_mm;
+ 	}
+ 	set_bit(MMF_VM_PINNED, &mm->flags);
+ 
+   done_mm:
+ 	up_write(&mm->mmap_sem);
+ 	mmput(mm);
+ 	return result;
+ }
+ 
+ EXPORT_SYMBOL(ipipe_disable_ondemand_mappings);
+ 
+ #endif
