***************
*** 4,9 ****
   *  Copyright (C) 1996,1997,1998 Russell King.
   *  ARM700 fix by Matthew Godbolt (linux-user@willothewisp.demon.co.uk)
   *  nommu support by Hyok S. Choi (hyok.choi@samsung.com)
   *
   * This program is free software; you can redistribute it and/or modify
   * it under the terms of the GNU General Public License version 2 as
--- 4,10 ----
   *  Copyright (C) 1996,1997,1998 Russell King.
   *  ARM700 fix by Matthew Godbolt (linux-user@willothewisp.demon.co.uk)
   *  nommu support by Hyok S. Choi (hyok.choi@samsung.com)
+  *  Copyright (C) 2005 Stelian Pop.
   *
   * This program is free software; you can redistribute it and/or modify
   * it under the terms of the GNU General Public License version 2 as
***************
*** 42,47 ****
  #endif
  	arch_irq_handler_default
  9997:
  	.endm
  
  #ifdef CONFIG_KPROBES
--- 43,67 ----
  #endif
  	arch_irq_handler_default
  9997:
+ #ifdef CONFIG_IPIPE
+ #if !defined(CONFIG_SMP)
+ 	ldr	r0, =ipipe_percpu_domain
+ 	ldr	r1, =ipipe_root
+ 	ldr	r0, [r0]
+ 	cmp	r0, r1
+ #if __GNUC__ >= 4
+ 	ldreq	r0, =__ipipe_root_status
+ #else /* gcc < 4 */
+ 	ldreq 	r0, =__ipipe_root_status_addr
+ 	ldreq	r0, [r0]
+ #endif /* gcc < 4 */
+ 	ldreq	r0, [r0]
+ 	tsteq	r0, #1
+ #else /* CONFIG_SMP */
+ 	bl	__ipipe_check_root_interruptible
+ 	cmp	r0, #1
+ #endif /* CONFIG_SMP */
+ #endif /* CONFIG_IPIPE */
  	.endm
  
  #ifdef CONFIG_KPROBES
***************
*** 62,67 ****
  	mov	r1, #\reason
  	.endm
  
  __pabt_invalid:
  	inv_entry BAD_PREFETCH
  	b	common_invalid
--- 82,102 ----
  	mov	r1, #\reason
  	.endm
  
+ 	.macro fcse_dabt_mva_to_va
+ #ifdef	CONFIG_ARM_FCSE
+ 	@
+ 	@ If FCSE is enabled the data abort fault address must be
+ 	@ converted from MVA to VA. This must happen before the irqs are
+ 	@ enabled if PREEMPT is enabled, as context switches may
+ 	@ change the FCSE pid.
+ 	@
+ 	mrc     p15, 0, r2, c13, c0, 0
+ 	eor	r2, r2, r0
+ 	tst	r2, #0xfe000000
+ 	moveq   r0, r2
+ #endif
+ 	.endm
+ 
  __pabt_invalid:
  	inv_entry BAD_PREFETCH
  	b	common_invalid
***************
*** 146,151 ****
  	@  r4 - orig_r0 (see pt_regs definition in ptrace.h)
  	@
  	stmia	r5, {r0 - r4}
  	.endm
  
  	.align	5
--- 181,195 ----
  	@  r4 - orig_r0 (see pt_regs definition in ptrace.h)
  	@
  	stmia	r5, {r0 - r4}
+ #ifdef CONFIG_IPIPE_TRACE_IRQSOFF
+ 	mov	r0, #1		/* IPIPE_TRACE_BEGIN */
+ 	mov	r3, #0x90000000
+ 	ldr	r2, [sp, #S_PC]
+ 	mov	r1, pc
+ 	bl	ipipe_trace_asm
+ 	ldmia	r5, {r0 - r4}
+ #endif /* CONFIG_IPIPE_TRACE_IRQSOFF */
+ 
  	.endm
  
  	.align	5
***************
*** 175,180 ****
  #else
  	bl	CPU_DABORT_HANDLER
  #endif
  
  	@
  	@ set desired IRQ state, then call main handler
--- 219,225 ----
  #else
  	bl	CPU_DABORT_HANDLER
  #endif
+ 	fcse_dabt_mva_to_va
  
  	@
  	@ set desired IRQ state, then call main handler
***************
*** 206,219 ****
  #endif
  #ifdef CONFIG_PREEMPT
  	get_thread_info tsk
  	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
  	add	r7, r8, #1			@ increment it
  	str	r7, [tsk, #TI_PREEMPT]
  #endif
  
  	irq_handler
  #ifdef CONFIG_PREEMPT
  	str	r8, [tsk, #TI_PREEMPT]		@ restore preempt count
  	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
  	teq	r8, #0				@ if preempt count != 0
  	movne	r0, #0				@ force flags to 0
--- 251,275 ----
  #endif
  #ifdef CONFIG_PREEMPT
  	get_thread_info tsk
+ #ifndef CONFIG_IPIPE
  	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
  	add	r7, r8, #1			@ increment it
  	str	r7, [tsk, #TI_PREEMPT]
  #endif
+ #endif
  
  	irq_handler
+ #ifdef CONFIG_IPIPE
+ 	ldrne	r4, [sp, #S_PSR]		@ irqs are already disabled
+ 	bne	__ipipe_fast_svc_irq_exit
+ #endif
+ 
  #ifdef CONFIG_PREEMPT
+ #ifndef	CONFIG_IPIPE
  	str	r8, [tsk, #TI_PREEMPT]		@ restore preempt count
+ #else
+ 	ldr	r8, [tsk, #TI_PREEMPT]
+ #endif
  	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
  	teq	r8, #0				@ if preempt count != 0
  	movne	r0, #0				@ force flags to 0
***************
*** 225,230 ****
  	tst	r4, #PSR_I_BIT
  	bleq	trace_hardirqs_on
  #endif
  	svc_exit r4				@ return from exception
   UNWIND(.fnend		)
  ENDPROC(__irq_svc)
--- 281,289 ----
  	tst	r4, #PSR_I_BIT
  	bleq	trace_hardirqs_on
  #endif
+ #ifdef CONFIG_IPIPE
+ __ipipe_fast_svc_irq_exit:
+ #endif
  	svc_exit r4				@ return from exception
   UNWIND(.fnend		)
  ENDPROC(__irq_svc)
***************
*** 234,245 ****
  #ifdef CONFIG_PREEMPT
  svc_preempt:
  	mov	r8, lr
  1:	bl	preempt_schedule_irq		@ irq en/disable is done inside
  	ldr	r0, [tsk, #TI_FLAGS]		@ get new tasks TI_FLAGS
  	tst	r0, #_TIF_NEED_RESCHED
  	moveq	pc, r8				@ go again
  	b	1b
- #endif
  
  	.align	5
  __und_svc:
--- 293,308 ----
  #ifdef CONFIG_PREEMPT
  svc_preempt:
  	mov	r8, lr
+ #ifndef CONFIG_IPIPE
  1:	bl	preempt_schedule_irq		@ irq en/disable is done inside
+ #else /* CONFIG_IPIPE */
+ 1:	bl	__ipipe_preempt_schedule_irq	@ irq en/disable is done inside
+ #endif /* CONFIG_IPIPE */
  	ldr	r0, [tsk, #TI_FLAGS]		@ get new tasks TI_FLAGS
  	tst	r0, #_TIF_NEED_RESCHED
  	moveq	pc, r8				@ go again
  	b	1b
+ #endif /* CONFIG_PREEMPT */
  
  	.align	5
  __und_svc:
***************
*** 252,257 ****
  	svc_entry
  #endif
  
  	@
  	@ call emulation code, which returns using r9 if it has emulated
  	@ the instruction, or the more conventional lr if we are to treat
--- 315,329 ----
  	svc_entry
  #endif
  
+ #ifdef CONFIG_IPIPE
+ 	mov	r4, r2
+ 	mov	r0, #7				@ r0 = IPIPE_TRAP_UNDEFINSTR
+ 	mov	r1, sp				@ r1 = &regs
+ 	bl	__ipipe_dispatch_event		@ branch to trap handler
+ 	cmp	r0, #0
+ 	bne	1f
+ 	mov	r2, r4
+ #endif /* CONFIG_IPIPE */
  	@
  	@ call emulation code, which returns using r9 if it has emulated
  	@ the instruction, or the more conventional lr if we are to treat
***************
*** 351,356 ****
   ARM(	stmib	sp, {r1 - r12}	)
   THUMB(	stmia	sp, {r0 - r12}	)
  
  	ldmia	r0, {r1 - r3}
  	add	r0, sp, #S_PC		@ here for interlock avoidance
  	mov	r4, #-1			@  ""  ""     ""        ""
--- 423,438 ----
   ARM(	stmib	sp, {r1 - r12}	)
   THUMB(	stmia	sp, {r0 - r12}	)
  
+ #ifdef CONFIG_IPIPE_TRACE_IRQSOFF
+ 	mov	r4, r0
+ 	mov	r0, #1		/* IPIPE_TRACE_BEGIN */
+ 	mov	r3, #0x90000000
+ 	ldr	r2, [r4, #4]	/* lr_<exception> */
+ 	mov	r1, pc
+ 	bl	ipipe_trace_asm
+ 	mov	r0, r4
+ #endif /* CONFIG_IPIPE_TRACE_IRQSOFF */
+ 
  	ldmia	r0, {r1 - r3}
  	add	r0, sp, #S_PC		@ here for interlock avoidance
  	mov	r4, #-1			@  ""  ""     ""        ""
***************
*** 380,385 ****
  	@ Clear FP to mark the first stack frame
  	@
  	zero_fp
  	.endm
  
  	.macro	kuser_cmpxchg_check
--- 462,468 ----
  	@ Clear FP to mark the first stack frame
  	@
  	zero_fp
+ 
  	.endm
  
  	.macro	kuser_cmpxchg_check
***************
*** 418,423 ****
  #else
  	bl	CPU_DABORT_HANDLER
  #endif
  
  	@
  	@ IRQs on, then call the main handler
--- 501,507 ----
  #else
  	bl	CPU_DABORT_HANDLER
  #endif
+ 	fcse_dabt_mva_to_va
  
  	@
  	@ IRQs on, then call the main handler
***************
*** 441,453 ****
  
  	get_thread_info tsk
  #ifdef CONFIG_PREEMPT
  	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
  	add	r7, r8, #1			@ increment it
  	str	r7, [tsk, #TI_PREEMPT]
  #endif
  
  	irq_handler
  #ifdef CONFIG_PREEMPT
  	ldr	r0, [tsk, #TI_PREEMPT]
  	str	r8, [tsk, #TI_PREEMPT]
  	teq	r0, r7
--- 525,545 ----
  
  	get_thread_info tsk
  #ifdef CONFIG_PREEMPT
+ #ifndef CONFIG_IPIPE
  	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
  	add	r7, r8, #1			@ increment it
  	str	r7, [tsk, #TI_PREEMPT]
  #endif
+ #endif
  
  	irq_handler
+ #ifdef CONFIG_IPIPE
+ 	beq	__ipipe_usr_irq_continue
+ 	slow_restore_user_regs 		@ Fast exit path over non-root domains
+ __ipipe_usr_irq_continue:
+ #endif
  #ifdef CONFIG_PREEMPT
+ #ifndef CONFIG_IPIPE
  	ldr	r0, [tsk, #TI_PREEMPT]
  	str	r8, [tsk, #TI_PREEMPT]
  	teq	r0, r7
***************
*** 455,460 ****
   THUMB(	movne	r0, #0		)
   THUMB(	strne	r0, [r0]	)
  #endif
  
  	mov	why, #0
  	b	ret_to_user_from_irq
--- 547,553 ----
   THUMB(	movne	r0, #0		)
   THUMB(	strne	r0, [r0]	)
  #endif
+ #endif
  
  	mov	why, #0
  	b	ret_to_user_from_irq
***************
*** 467,472 ****
  __und_usr:
  	usr_entry
  
  	@
  	@ fall through to the emulation code, which returns using r9 if
  	@ it has emulated the instruction, or the more conventional lr
--- 560,576 ----
  __und_usr:
  	usr_entry
  
+ #ifdef CONFIG_IPIPE
+ 	mov	r0, #7				@ r0 = IPIPE_TRAP_UNDEFINSTR
+ 	mov	r1, sp				@ r1 = &regs
+ 	mov	r4, r2
+ 	mov	r5, r3
+ 	bl	__ipipe_dispatch_event		@ branch to trap handler
+ 	cmp	r0, #0
+ 	bne	ret_from_exception
+ 	mov	r2, r4
+ 	mov	r3, r5
+ #endif /* CONFIG_IPIPE */
  	@
  	@ fall through to the emulation code, which returns using r9 if
  	@ it has emulated the instruction, or the more conventional lr
***************
*** 703,708 ****
  ENTRY(ret_from_exception)
   UNWIND(.fnstart	)
   UNWIND(.cantunwind	)
  	get_thread_info tsk
  	mov	why, #0
  	b	ret_to_user
--- 807,827 ----
  ENTRY(ret_from_exception)
   UNWIND(.fnstart	)
   UNWIND(.cantunwind	)
+ #ifdef CONFIG_IPIPE
+ 	disable_irq
+ #if !defined(CONFIG_SMP)
+ 	ldr	r0, =ipipe_percpu_domain
+ 	ldr	r1, =ipipe_root
+ 	ldr	r0, [r0]
+ 	cmp	r0, r1
+ #else /* CONFIG_SMP */
+ 	bl     __ipipe_check_root
+ 	cmp     r0, #1
+ #endif /* CONFIG_SMP */
+ 	beq     1f
+ 	slow_restore_user_regs          @ Fast exit path over non-root domains
+ 1:
+ #endif /* CONFIG_IPIPE */
  	get_thread_info tsk
  	mov	why, #0
  	b	ret_to_user
***************
*** 740,746 ****
  	add	r4, r2, #TI_CPU_SAVE
  	ldr	r0, =thread_notify_head
  	mov	r1, #THREAD_NOTIFY_SWITCH
  	bl	atomic_notifier_call_chain
  #if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)
  	str	r7, [r8]
  #endif
--- 859,869 ----
  	add	r4, r2, #TI_CPU_SAVE
  	ldr	r0, =thread_notify_head
  	mov	r1, #THREAD_NOTIFY_SWITCH
+ #ifndef CONFIG_IPIPE
  	bl	atomic_notifier_call_chain
+ #else /* CONFIG_IPIPE */
+ 	bl	__ipipe_switch_to_notifier_call_chain
+ #endif /* CONFIG_IPIPE */
  #if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)
  	str	r7, [r8]
  #endif
***************
*** 794,799 ****
  #endif
  	.endm
  
  	.align	5
  	.globl	__kuser_helper_start
  __kuser_helper_start:
--- 917,966 ----
  #endif
  	.endm
  
+ #ifdef CONFIG_IPIPE
+ /*
+ 	I-pipe tsc area, here we store data shared with user-space for
+ 	tsc-emulation. If CONFIG_IPIPE_ARM_KUSER_TSC is enabled
+ 	__ipipe_kuser_get_tsc will be overwritten with the real TSC
+ 	emulation code.
+ */
+ 	.globl	__ipipe_tsc_area
+ 	.equ	__ipipe_tsc_area, CONFIG_VECTORS_BASE + 0x1000 + __ipipe_tsc_area_start - __kuser_helper_end
+ 
+ #ifdef CONFIG_IPIPE_ARM_KUSER_TSC
+ 	.globl  __ipipe_tsc_addr
+ 	.equ	__ipipe_tsc_addr, CONFIG_VECTORS_BASE + 0x1000 + .LCcntr_addr - __kuser_helper_end
+ 
+ 	.globl	__ipipe_tsc_get
+ 	.equ	__ipipe_tsc_get, CONFIG_VECTORS_BASE + 0x1000 + __ipipe_kuser_get_tsc - __kuser_helper_end
+ #endif
+ 
+ 	.align 5
+ 	.globl  __ipipe_tsc_area_start
+ __ipipe_tsc_area_start:
+ 	.rep  3
+ 	.word 0
+ 	.endr
+ 
+ #ifdef CONFIG_IPIPE_ARM_KUSER_TSC
+ 	.rep  4
+ 	.word 0
+ 	.endr
+ .LCcntr_addr:
+ 	.word 0
+ 
+ 	.align 5
+ __ipipe_kuser_get_tsc:
+ 	nop
+ 	mov	r0, #0
+ 	mov	r1, #0
+ 	usr_ret	lr
+ 	.rep 20
+ 	.word 0
+ 	.endr
+ #endif
+ #endif
+ 
  	.align	5
  	.globl	__kuser_helper_start
  __kuser_helper_start:
***************
*** 827,833 ****
   *
   * #define __kernel_dmb() \
   *         asm volatile ( "mov r0, #0xffff0fff; mov lr, pc; sub pc, r0, #95" \
-  *	        : : : "r0", "lr","cc" )
   */
  
  __kuser_memory_barrier:				@ 0xffff0fa0
--- 994,1000 ----
   *
   * #define __kernel_dmb() \
   *         asm volatile ( "mov r0, #0xffff0fff; mov lr, pc; sub pc, r0, #95" \
+  *		: : : "r0", "lr","cc" )
   */
  
  __kuser_memory_barrier:				@ 0xffff0fa0
***************
*** 992,998 ****
   * #define __kernel_get_tls() \
   *	({ register unsigned int __val asm("r0"); \
   *         asm( "mov r0, #0xffff0fff; mov lr, pc; sub pc, r0, #31" \
-  *	        : "=r" (__val) : : "lr","cc" ); \
   *	   __val; })
   */
  
--- 1159,1165 ----
   * #define __kernel_get_tls() \
   *	({ register unsigned int __val asm("r0"); \
   *         asm( "mov r0, #0xffff0fff; mov lr, pc; sub pc, r0, #31" \
+  *		: "=r" (__val) : : "lr","cc" ); \
   *	   __val; })
   */
  
