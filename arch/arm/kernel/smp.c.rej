***************
*** 53,58 ****
  	IPI_CALL_FUNC,
  	IPI_CALL_FUNC_SINGLE,
  	IPI_CPU_STOP,
  };
  
  int __cpuinit __cpu_up(unsigned int cpu)
--- 53,77 ----
  	IPI_CALL_FUNC,
  	IPI_CALL_FUNC_SINGLE,
  	IPI_CPU_STOP,
+ 	IPI_CPU_DUMP,
+ #ifdef CONFIG_IPIPE
+ 	IPI_IPIPE_CRITICAL,
+ 	IPI_IPIPE_0,
+ 	IPI_IPIPE_1,
+ 	IPI_IPIPE_2,
+ 	IPI_IPIPE_3,
+ 	IPI_IPIPE_VNMI,
+ #define IPI_IPIPE_ALL				\
+ 	((1UL << IPI_IPIPE_CRITICAL)|		\
+ 	 (1UL << IPI_IPIPE_0)|			\
+ 	 (1UL << IPI_IPIPE_1)|			\
+ 	 (1UL << IPI_IPIPE_2)|			\
+ 	 (1UL << IPI_IPIPE_3)|			\
+ 	 (1UL << IPI_IPIPE_VNMI))
+ #define IPI_ROOT_MASK  IPI_IPIPE_ALL
+ #else /* !CONFIG_IPIPE */
+ #define IPI_ROOT_MASK  0
+ #endif /* !CONFIG_IPIPE */
  };
  
  int __cpuinit __cpu_up(unsigned int cpu)
***************
*** 286,292 ****
  	atomic_inc(&mm->mm_count);
  	current->active_mm = mm;
  	cpumask_set_cpu(cpu, mm_cpumask(mm));
- 	cpu_switch_mm(mm->pgd, mm);
  	enter_lazy_tlb(mm, current);
  	local_flush_tlb_all();
  
--- 305,311 ----
  	atomic_inc(&mm->mm_count);
  	current->active_mm = mm;
  	cpumask_set_cpu(cpu, mm_cpumask(mm));
+ 	cpu_switch_mm(mm->pgd, mm, 1);
  	enter_lazy_tlb(mm, current);
  	local_flush_tlb_all();
  
***************
*** 445,450 ****
  static void ipi_timer(void)
  {
  	struct clock_event_device *evt = &__get_cpu_var(percpu_clockevent);
  	evt->event_handler(evt);
  }
  
--- 464,478 ----
  static void ipi_timer(void)
  {
  	struct clock_event_device *evt = &__get_cpu_var(percpu_clockevent);
+ 
+ #ifdef CONFIG_IPIPE
+ #ifndef CONFIG_IPIPE_ARM_KUSER_TSC
+ 	__ipipe_mach_update_tsc();
+ #else /* CONFIG_IPIPE_ARM_KUSER_TSC */
+ 	__ipipe_tsc_update();
+ #endif /* CONFIG_IPIPE_ARM_KUSER_TSC */
+ #endif /* CONFIG_IPIPE */
+ 
  	evt->event_handler(evt);
  }
  
***************
*** 477,482 ****
  }
  #endif
  
  #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
  static void smp_timer_broadcast(const struct cpumask *mask)
  {
--- 505,565 ----
  }
  #endif
  
+ #ifdef CONFIG_IPIPE
+ 
+ void __ipipe_mach_release_timer(void)
+ {
+ 	int cpu = ipipe_processor_id();
+ 	struct clock_event_device *evt = &per_cpu(percpu_clockevent, cpu);
+ 
+ 	evt->set_mode(evt->mode, evt);
+ 	if (evt->mode == CLOCK_EVT_MODE_ONESHOT)
+ 		evt->set_next_event(__ipipe_mach_ticks_per_jiffy, evt);
+ }
+ EXPORT_SYMBOL(__ipipe_mach_release_timer);
+ 
+ int __ipipe_send_ipi(unsigned ipi, cpumask_t cpumask)
+ {
+ 	enum ipi_msg_type msg = ipi - IPIPE_FIRST_IPI + IPI_IPIPE_CRITICAL;
+ 	smp_cross_call(&cpumask, msg);
+ 	return 0;
+ }
+ 
+ asmlinkage void __exception
+ __ipipe_grab_ipi(unsigned svc, struct pt_regs *regs) /* hw IRQs off */
+ {
+ 	unsigned int cpu = ipipe_processor_id();
+ 	int virq;
+ 
+ 	/*
+ 	 * Virtual NMIs ignore the root domain's stall
+ 	 * bit. When caught over high priority
+ 	 * domains, virtual VMIs are pipelined the
+ 	 * usual way as normal interrupts.
+ 	 */
+ 	if (svc == IPI_IPIPE_VNMI && ipipe_root_domain_p)
+ 		__ipipe_do_vnmi(IPIPE_SERVICE_VNMI, NULL);
+ 	else if ((1 << svc) & IPI_IPIPE_ALL) {
+ 		virq = svc - IPI_IPIPE_CRITICAL + IPIPE_FIRST_IPI;
+ 		__ipipe_handle_irq(virq, IPIPE_IRQF_NOACK);
+ 	} else
+ 		__ipipe_mach_relay_ipi(svc, cpu);
+ 
+ 	__ipipe_exit_irq(regs);
+ }
+ 
+ void  __ipipe_root_ipi(unsigned int ipinr, void *cookie)
+ {
+ 	do_IPI(ipinr, &__raw_get_cpu_var(__ipipe_tick_regs));
+ }
+ 
+ void  __ipipe_root_localtimer(unsigned int irq, void *cookie)
+ {
+ 	do_local_timer(&__raw_get_cpu_var(__ipipe_tick_regs));
+ }
+ 
+ #endif /* CONFIG_IPIPE */
+ 
  #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
  static void smp_timer_broadcast(const struct cpumask *mask)
  {
