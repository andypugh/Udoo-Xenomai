***************
*** 29,34 ****
  static pteval_t shared_pte_mask = L_PTE_MT_BUFFERABLE;
  
  #if __LINUX_ARM_ARCH__ < 6
  /*
   * We take the easy way out of this problem - we make the
   * PTE uncacheable.  However, we leave the write buffer on.
--- 29,58 ----
  static pteval_t shared_pte_mask = L_PTE_MT_BUFFERABLE;
  
  #if __LINUX_ARM_ARCH__ < 6
+ #ifdef CONFIG_ARM_FCSE_BEST_EFFORT
+ static void fcse_set_pte_shared(struct vm_area_struct *vma,
+ 				unsigned long address, pte_t *ptep)
+ {
+ 	pte_t entry;
+ 
+ 	if (!(vma->vm_flags & VM_MAYSHARE) || address >= TASK_SIZE)
+ 		return;
+ 
+ 	entry = *ptep;
+ 	if ((pte_val(entry)
+ 	     & (L_PTE_PRESENT | PTE_CACHEABLE | L_PTE_RDONLY | L_PTE_DIRTY | L_PTE_SHARED))
+ 	    == (L_PTE_PRESENT | PTE_CACHEABLE | L_PTE_DIRTY)) {
+ 		pte_val(entry) |= L_PTE_SHARED;
+ 		/* Bypass set_pte_at here, we are not changing
+ 		   hardware bits, flush is not needed */
+ 		++vma->vm_mm->context.fcse.shared_dirty_pages;
+ 		*ptep = entry;
+ 	}
+ }
+ #else /* !CONFIG_ARM_FCSE_BEST_EFFORT */
+ #define fcse_set_pte_shared(vma, addr, ptep) do { } while (0)
+ #endif /* !CONFIG_ARM_FCSE_BEST_EFFORT */
+ 
  /*
   * We take the easy way out of this problem - we make the
   * PTE uncacheable.  However, we leave the write buffer on.
***************
*** 66,71 ****
  	return ret;
  }
  
  #if USE_SPLIT_PTLOCKS
  /*
   * If we are using split PTE locks, then we need to take the page
--- 90,96 ----
  	return ret;
  }
  
+ #ifndef CONFIG_ARM_FCSE_GUARANTEED
  #if USE_SPLIT_PTLOCKS
  /*
   * If we are using split PTE locks, then we need to take the page
***************
*** 128,138 ****
  
  	return ret;
  }
  
  static void
  make_coherent(struct address_space *mapping, struct vm_area_struct *vma,
  	unsigned long addr, pte_t *ptep, unsigned long pfn)
  {
  	struct mm_struct *mm = vma->vm_mm;
  	struct vm_area_struct *mpnt;
  	struct prio_tree_iter iter;
--- 153,165 ----
  
  	return ret;
  }
+ #endif /* CONFIG_ARM_FCSE_GUARANTEED */
  
  static void
  make_coherent(struct address_space *mapping, struct vm_area_struct *vma,
  	unsigned long addr, pte_t *ptep, unsigned long pfn)
  {
+ #ifndef CONFIG_ARM_FCSE_GUARANTEED
  	struct mm_struct *mm = vma->vm_mm;
  	struct vm_area_struct *mpnt;
  	struct prio_tree_iter iter;
***************
*** 164,169 ****
  	flush_dcache_mmap_unlock(mapping);
  	if (aliases)
  		do_adjust_pte(vma, addr, pfn, ptep);
  }
  
  /*
--- 191,202 ----
  	flush_dcache_mmap_unlock(mapping);
  	if (aliases)
  		do_adjust_pte(vma, addr, pfn, ptep);
+ 	else
+ 		fcse_set_pte_shared(vma, addr, ptep);
+ #else /* CONFIG_ARM_FCSE_GUARANTEED */
+ 	if (vma->vm_flags & VM_MAYSHARE)
+ 		do_adjust_pte(vma, addr, pfn, ptep);
+ #endif /* CONFIG_ARM_FCSE_GUARANTEED */
  }
  
  /*
