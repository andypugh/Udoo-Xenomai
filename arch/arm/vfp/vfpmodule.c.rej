***************
*** 8,14 ****
   * it under the terms of the GNU General Public License version 2 as
   * published by the Free Software Foundation.
   */
- #include <linux/module.h>
  #include <linux/types.h>
  #include <linux/cpu.h>
  #include <linux/kernel.h>
--- 8,13 ----
   * it under the terms of the GNU General Public License version 2 as
   * published by the Free Software Foundation.
   */
  #include <linux/types.h>
  #include <linux/cpu.h>
  #include <linux/kernel.h>
***************
*** 33,39 ****
  void vfp_null_entry(void);
  
  void (*vfp_vector)(void) = vfp_null_entry;
- union vfp_state *last_VFP_context[NR_CPUS];
  
  /*
   * Dual-use variable.
--- 32,37 ----
  void vfp_null_entry(void);
  
  void (*vfp_vector)(void) = vfp_null_entry;
  
  /*
   * Dual-use variable.
***************
*** 43,81 ****
  unsigned int VFP_arch;
  
  /*
   * Per-thread VFP initialization.
   */
  static void vfp_thread_flush(struct thread_info *thread)
  {
  	union vfp_state *vfp = &thread->vfpstate;
  	unsigned int cpu;
  
- 	memset(vfp, 0, sizeof(union vfp_state));
- 
- 	vfp->hard.fpexc = FPEXC_EN;
- 	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
- 
  	/*
  	 * Disable VFP to ensure we initialize it first.  We must ensure
- 	 * that the modification of last_VFP_context[] and hardware disable
- 	 * are done for the same CPU and without preemption.
  	 */
- 	cpu = get_cpu();
- 	if (last_VFP_context[cpu] == vfp)
- 		last_VFP_context[cpu] = NULL;
  	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
- 	put_cpu();
  }
  
  static void vfp_thread_exit(struct thread_info *thread)
  {
  	/* release case: Per-thread VFP cleanup. */
  	union vfp_state *vfp = &thread->vfpstate;
- 	unsigned int cpu = get_cpu();
  
- 	if (last_VFP_context[cpu] == vfp)
- 		last_VFP_context[cpu] = NULL;
- 	put_cpu();
  }
  
  static void vfp_thread_copy(struct thread_info *thread)
--- 41,127 ----
  unsigned int VFP_arch;
  
  /*
+  * The pointer to the vfpstate structure of the thread which currently
+  * owns the context held in the VFP hardware, or NULL if the hardware
+  * context is invalid.
+  *
+  * For UP, this is sufficient to tell which thread owns the VFP context.
+  * However, for SMP, we also need to check the CPU number stored in the
+  * saved state too to catch migrations.
+  */
+ union vfp_state *vfp_current_hw_state[NR_CPUS];
+ 
+ /*
+  * Is 'thread's most up to date state stored in this CPUs hardware?
+  * Must be called from non-preemptible context.
+  */
+ static bool vfp_state_in_hw(unsigned int cpu, struct thread_info *thread)
+ {
+ #ifdef CONFIG_SMP
+ 	if (thread->vfpstate.hard.cpu != cpu)
+ 		return false;
+ #endif
+ 	return vfp_current_hw_state[cpu] == &thread->vfpstate;
+ }
+ 
+ /*
+  * Force a reload of the VFP context from the thread structure.  We do
+  * this by ensuring that access to the VFP hardware is disabled, and
+  * clear vfp_current_hw_state.  Must be called from non-preemptible context.
+  */
+ static void vfp_force_reload(unsigned int cpu, struct thread_info *thread)
+ {
+ 	if (vfp_state_in_hw(cpu, thread)) {
+ 		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+ 		vfp_current_hw_state[cpu] = NULL;
+ 	}
+ #ifdef CONFIG_SMP
+ 	thread->vfpstate.hard.cpu = NR_CPUS;
+ #endif
+ }
+ 
+ /*
   * Per-thread VFP initialization.
   */
  static void vfp_thread_flush(struct thread_info *thread)
  {
  	union vfp_state *vfp = &thread->vfpstate;
+ 	unsigned long flags;
  	unsigned int cpu;
  
  	/*
  	 * Disable VFP to ensure we initialize it first.  We must ensure
+ 	 * that the modification of vfp_current_hw_state[] and hardware
+ 	 * disable are done for the same CPU and without preemption.
+ 	 *
+ 	 * Do this first to ensure that preemption won't overwrite our
+ 	 * state saving should access to the VFP be enabled at this point.
  	 */
+ 	cpu = ipipe_get_cpu(flags);
+ 	if (vfp_current_hw_state[cpu] == vfp)
+ 		vfp_current_hw_state[cpu] = NULL;
  	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+ 
+ 	memset(vfp, 0, sizeof(union vfp_state));
+ 
+ 	vfp->hard.fpexc = FPEXC_EN;
+ 	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
+ #ifdef CONFIG_SMP
+ 	vfp->hard.cpu = NR_CPUS;
+ #endif
+ 	ipipe_put_cpu(flags);
  }
  
  static void vfp_thread_exit(struct thread_info *thread)
  {
  	/* release case: Per-thread VFP cleanup. */
  	union vfp_state *vfp = &thread->vfpstate;
+ 	unsigned long flags;
+ 	unsigned int cpu = ipipe_get_cpu(flags);
  
+ 	if (vfp_current_hw_state[cpu] == vfp)
+ 		vfp_current_hw_state[cpu] = NULL;
+ 	ipipe_put_cpu(flags);
  }
  
  static void vfp_thread_copy(struct thread_info *thread)
***************
*** 84,89 ****
  
  	vfp_sync_hwstate(parent);
  	thread->vfpstate = parent->vfpstate;
  }
  
  /*
--- 130,138 ----
  
  	vfp_sync_hwstate(parent);
  	thread->vfpstate = parent->vfpstate;
+ #ifdef CONFIG_SMP
+ 	thread->vfpstate.hard.cpu = NR_CPUS;
+ #endif
  }
  
  /*
***************
*** 112,117 ****
  static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
  {
  	struct thread_info *thread = v;
  	u32 fpexc;
  #ifdef CONFIG_SMP
  	unsigned int cpu;
--- 161,167 ----
  static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
  {
  	struct thread_info *thread = v;
+ 	unsigned long flags;
  	u32 fpexc;
  #ifdef CONFIG_SMP
  	unsigned int cpu;
***************
*** 119,126 ****
  
  	switch (cmd) {
  	case THREAD_NOTIFY_SWITCH:
- 		fpexc = fmrx(FPEXC);
  
  #ifdef CONFIG_SMP
  		cpu = thread->cpu;
  
--- 169,177 ----
  
  	switch (cmd) {
  	case THREAD_NOTIFY_SWITCH:
  
+ 		local_irq_save_hw_cond(flags);
+ 		fpexc = fmrx(FPEXC);
  #ifdef CONFIG_SMP
  		cpu = thread->cpu;
  
***************
*** 129,145 ****
  		 * case the thread migrates to a different CPU. The
  		 * restoring is done lazily.
  		 */
- 		if ((fpexc & FPEXC_EN) && last_VFP_context[cpu]) {
- 			vfp_save_state(last_VFP_context[cpu], fpexc);
- 			last_VFP_context[cpu]->hard.cpu = cpu;
- 		}
- 		/*
- 		 * Thread migration, just force the reloading of the
- 		 * state on the new CPU in case the VFP registers
- 		 * contain stale data.
- 		 */
- 		if (thread->vfpstate.hard.cpu != cpu)
- 			last_VFP_context[cpu] = NULL;
  #endif
  
  		/*
--- 180,187 ----
  		 * case the thread migrates to a different CPU. The
  		 * restoring is done lazily.
  		 */
+ 		if ((fpexc & FPEXC_EN) && vfp_current_hw_state[cpu])
+ 			vfp_save_state(vfp_current_hw_state[cpu], fpexc);
  #endif
  
  		/*
***************
*** 147,152 ****
  		 * old state.
  		 */
  		fmxr(FPEXC, fpexc & ~FPEXC_EN);
  		break;
  
  	case THREAD_NOTIFY_FLUSH:
--- 189,195 ----
  		 * old state.
  		 */
  		fmxr(FPEXC, fpexc & ~FPEXC_EN);
+ 		local_irq_restore_hw_cond(flags);
  		break;
  
  	case THREAD_NOTIFY_FLUSH:
***************
*** 158,164 ****
  		break;
  
  	case THREAD_NOTIFY_COPY:
  		vfp_thread_copy(thread);
  		break;
  	}
  
--- 201,209 ----
  		break;
  
  	case THREAD_NOTIFY_COPY:
+ 		local_irq_save_hw_cond(flags);
  		vfp_thread_copy(thread);
+ 		local_irq_restore_hw_cond(flags);
  		break;
  	}
  
***************
*** 290,296 ****
   */
  void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
  {
- 	u32 fpscr, orig_fpscr, fpsid, exceptions;
  
  	pr_debug("VFP: bounce: trigger %08x fpexc %08x\n", trigger, fpexc);
  
--- 335,341 ----
   */
  void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
  {
+ 	u32 fpscr, orig_fpscr, fpsid, exceptions, next_trigger = 0;
  
  	pr_debug("VFP: bounce: trigger %08x fpexc %08x\n", trigger, fpexc);
  
***************
*** 320,325 ****
  		/*
  		 * Synchronous exception, emulate the trigger instruction
  		 */
  		goto emulate;
  	}
  
--- 365,371 ----
  		/*
  		 * Synchronous exception, emulate the trigger instruction
  		 */
+ 		local_irq_enable_hw_cond();
  		goto emulate;
  	}
  
***************
*** 332,338 ****
  		trigger = fmrx(FPINST);
  		regs->ARM_pc -= 4;
  #endif
- 	} else if (!(fpexc & FPEXC_DEX)) {
  		/*
  		 * Illegal combination of bits. It can be caused by an
  		 * unallocated VFP instruction but with FPSCR.IXE set and not
--- 378,395 ----
  		trigger = fmrx(FPINST);
  		regs->ARM_pc -= 4;
  #endif
+ 		if (fpexc & FPEXC_FP2V) {
+ 			/*
+ 			 * The barrier() here prevents fpinst2 being read
+ 			 * before the condition above.
+ 			 */
+ 			barrier();
+ 			next_trigger = fmrx(FPINST2);
+ 		}
+ 	}
+ 	local_irq_enable_hw_cond();
+ 
+ 	if (!(fpexc & (FPEXC_EX | FPEXC_DEX))) {
  		/*
  		 * Illegal combination of bits. It can be caused by an
  		 * unallocated VFP instruction but with FPSCR.IXE set and not
***************
*** 372,389 ****
  	if (fpexc ^ (FPEXC_EX | FPEXC_FP2V))
  		goto exit;
  
- 	/*
- 	 * The barrier() here prevents fpinst2 being read
- 	 * before the condition above.
- 	 */
- 	barrier();
- 	trigger = fmrx(FPINST2);
  
   emulate:
  	exceptions = vfp_emulate_instruction(trigger, orig_fpscr, regs);
  	if (exceptions)
  		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
   exit:
  	preempt_enable();
  }
  
--- 429,442 ----
  	if (fpexc ^ (FPEXC_EX | FPEXC_FP2V))
  		goto exit;
  
+ 	trigger = next_trigger;
  
   emulate:
  	exceptions = vfp_emulate_instruction(trigger, orig_fpscr, regs);
  	if (exceptions)
  		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
   exit:
+ 	local_irq_enable_hw_cond();
  	preempt_enable();
  }
  
***************
*** 397,405 ****
  	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
  }
  
- #ifdef CONFIG_PM
- #include <linux/syscore_ops.h>
- 
  static int vfp_pm_suspend(void)
  {
  	struct thread_info *ti = current_thread_info();
--- 450,456 ----
  	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
  }
  
+ #ifdef CONFIG_CPU_PM
  static int vfp_pm_suspend(void)
  {
  	struct thread_info *ti = current_thread_info();
***************
*** 415,421 ****
  	}
  
  	/* clear any information we had about last context state */
- 	memset(last_VFP_context, 0, sizeof(last_VFP_context));
  
  	return 0;
  }
--- 466,472 ----
  	}
  
  	/* clear any information we had about last context state */
+ 	memset(vfp_current_hw_state, 0, sizeof(vfp_current_hw_state));
  
  	return 0;
  }
***************
*** 429,457 ****
  	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
  }
  
- static struct syscore_ops vfp_pm_syscore_ops = {
- 	.suspend	= vfp_pm_suspend,
- 	.resume		= vfp_pm_resume,
  };
  
  static void vfp_pm_init(void)
  {
- 	register_syscore_ops(&vfp_pm_syscore_ops);
  }
  
  #else
  static inline void vfp_pm_init(void) { }
- #endif /* CONFIG_PM */
  
  void vfp_sync_hwstate(struct thread_info *thread)
  {
- 	unsigned int cpu = get_cpu();
  
- 	/*
- 	 * If the thread we're interested in is the current owner of the
- 	 * hardware VFP state, then we need to save its state.
- 	 */
- 	if (last_VFP_context[cpu] == &thread->vfpstate) {
  		u32 fpexc = fmrx(FPEXC);
  
  		/*
--- 480,523 ----
  	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
  }
  
+ static int vfp_cpu_pm_notifier(struct notifier_block *self, unsigned long cmd,
+ 	void *v)
+ {
+ 	switch (cmd) {
+ 	case CPU_PM_ENTER:
+ 		vfp_pm_suspend();
+ 		break;
+ 	case CPU_PM_ENTER_FAILED:
+ 	case CPU_PM_EXIT:
+ 		vfp_pm_resume();
+ 		break;
+ 	}
+ 	return NOTIFY_OK;
+ }
+ 
+ static struct notifier_block vfp_cpu_pm_notifier_block = {
+ 	.notifier_call = vfp_cpu_pm_notifier,
  };
  
  static void vfp_pm_init(void)
  {
+ 	cpu_pm_register_notifier(&vfp_cpu_pm_notifier_block);
  }
  
  #else
  static inline void vfp_pm_init(void) { }
+ #endif /* CONFIG_CPU_PM */
  
+ /*
+  * Ensure that the VFP state stored in 'thread->vfpstate' is up to date
+  * with the hardware state.
+  */
  void vfp_sync_hwstate(struct thread_info *thread)
  {
+ 	unsigned long flags;
+ 	unsigned int cpu = ipipe_get_cpu(flags);
  
+ 	if (vfp_state_in_hw(cpu, thread)) {
  		u32 fpexc = fmrx(FPEXC);
  
  		/*
***************
*** 462,501 ****
  		fmxr(FPEXC, fpexc);
  	}
  
- 	put_cpu();
  }
  
  void vfp_flush_hwstate(struct thread_info *thread)
  {
- 	unsigned int cpu = get_cpu();
- 
- 	/*
- 	 * If the thread we're interested in is the current owner of the
- 	 * hardware VFP state, then we need to save its state.
- 	 */
- 	if (last_VFP_context[cpu] == &thread->vfpstate) {
- 		u32 fpexc = fmrx(FPEXC);
  
- 		fmxr(FPEXC, fpexc & ~FPEXC_EN);
- 
- 		/*
- 		 * Set the context to NULL to force a reload the next time
- 		 * the thread uses the VFP.
- 		 */
- 		last_VFP_context[cpu] = NULL;
- 	}
  
- #ifdef CONFIG_SMP
- 	/*
- 	 * For SMP we still have to take care of the case where the thread
- 	 * migrates to another CPU and then back to the original CPU on which
- 	 * the last VFP user is still the same thread. Mark the thread VFP
- 	 * state as belonging to a non-existent CPU so that the saved one will
- 	 * be reloaded in the above case.
- 	 */
- 	thread->vfpstate.hard.cpu = NR_CPUS;
- #endif
- 	put_cpu();
  }
  
  /*
--- 528,545 ----
  		fmxr(FPEXC, fpexc);
  	}
  
+ 	ipipe_put_cpu(flags);
  }
  
+ /* Ensure that the thread reloads the hardware VFP state on the next use. */
  void vfp_flush_hwstate(struct thread_info *thread)
  {
+ 	unsigned long flags;
+ 	unsigned int cpu = ipipe_get_cpu(flags);
  
+ 	vfp_force_reload(cpu, thread);
  
+ 	ipipe_put_cpu(flags);
  }
  
  /*
***************
*** 513,520 ****
  	void *hcpu)
  {
  	if (action == CPU_DYING || action == CPU_DYING_FROZEN) {
- 		unsigned int cpu = (long)hcpu;
- 		last_VFP_context[cpu] = NULL;
  	} else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
  		vfp_enable(NULL);
  	return NOTIFY_OK;
--- 557,563 ----
  	void *hcpu)
  {
  	if (action == CPU_DYING || action == CPU_DYING_FROZEN) {
+ 		vfp_force_reload((long)hcpu, current_thread_info());
  	} else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
  		vfp_enable(NULL);
  	return NOTIFY_OK;
***************
*** 582,588 ****
  				elf_hwcap |= HWCAP_VFPv3D16;
  		}
  #endif
- #ifdef CONFIG_NEON
  		/*
  		 * Check for the presence of the Advanced SIMD
  		 * load/store instructions, integer and single
--- 625,630 ----
  				elf_hwcap |= HWCAP_VFPv3D16;
  		}
  #endif
  		/*
  		 * Check for the presence of the Advanced SIMD
  		 * load/store instructions, integer and single
***************
*** 590,599 ****
  		 * for NEON if the hardware has the MVFR registers.
  		 */
  		if ((read_cpuid_id() & 0x000f0000) == 0x000f0000) {
  			if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
  				elf_hwcap |= HWCAP_NEON;
- 		}
  #endif
  	}
  	return 0;
  }
--- 632,646 ----
  		 * for NEON if the hardware has the MVFR registers.
  		 */
  		if ((read_cpuid_id() & 0x000f0000) == 0x000f0000) {
+ #ifdef CONFIG_NEON
  			if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
  				elf_hwcap |= HWCAP_NEON;
  #endif
+ #if 0
+ 			if ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)
+ 				elf_hwcap |= HWCAP_VFPv4;
+ #endif
+ 		}
  	}
  	return 0;
  }
