***************
*** 11,21 ****
  #define _ASMARM_CACHEFLUSH_H
  
  #include <linux/mm.h>
  
  #include <asm/glue-cache.h>
  #include <asm/shmparam.h>
  #include <asm/cachetype.h>
  #include <asm/outercache.h>
  
  #define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
  
--- 11,23 ----
  #define _ASMARM_CACHEFLUSH_H
  
  #include <linux/mm.h>
+ #include <linux/sched.h>
  
  #include <asm/glue-cache.h>
  #include <asm/shmparam.h>
  #include <asm/cachetype.h>
  #include <asm/outercache.h>
+ #include <asm/fcse.h>
  
  #define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
  
***************
*** 157,162 ****
  
  #endif
  
  /*
   * Copy user data from/to a page which is mapped into a different
   * processes address space.  Really, we want to allow our "user
--- 159,185 ----
  
  #endif
  
+ #ifdef CONFIG_ARM_FCSE
+ #define FCSE_CACHE_MASK (~(L1_CACHE_BYTES - 1))
+ #define FCSE_CACHE_ALIGN(addr) (((addr) + ~FCSE_CACHE_MASK) & FCSE_CACHE_MASK)
+ 
+ static inline void
+ fcse_flush_cache_user_range(struct vm_area_struct *vma,
+ 			    unsigned long start, unsigned long end)
+ {
+ 	if (cache_is_vivt()
+ 	    && fcse_mm_in_cache(vma->vm_mm)) {
+ 		start = fcse_va_to_mva(vma->vm_mm, start & FCSE_CACHE_MASK);
+ 		end = fcse_va_to_mva(vma->vm_mm, FCSE_CACHE_ALIGN(end));
+ 		__cpuc_flush_user_range(start, end, vma->vm_flags);
+ 	}
+ }
+ #undef FCSE_CACHE_MASK
+ #undef FCSE_CACHE_ALIGN
+ #else /* ! CONFIG_ARM_FCSE */
+ #define fcse_flush_cache_user_range(vma, start, end) do { } while (0)
+ #endif /* ! CONFIG_ARM_FCSE */
+ 
  /*
   * Copy user data from/to a page which is mapped into a different
   * processes address space.  Really, we want to allow our "user
***************
*** 164,172 ****
   */
  extern void copy_to_user_page(struct vm_area_struct *, struct page *,
  	unsigned long, void *, const void *, unsigned long);
- #define copy_from_user_page(vma, page, vaddr, dst, src, len) \
- 	do {							\
- 		memcpy(dst, src, len);				\
  	} while (0)
  
  /*
--- 187,196 ----
   */
  extern void copy_to_user_page(struct vm_area_struct *, struct page *,
  	unsigned long, void *, const void *, unsigned long);
+ #define copy_from_user_page(vma, page, vaddr, dst, src, len)		\
+ 	do {								\
+ 		fcse_flush_cache_user_range(vma, vaddr, vaddr + len);	\
+ 		memcpy(dst, src, len);					\
  	} while (0)
  
  /*
***************
*** 208,215 ****
  
  static inline void vivt_flush_cache_mm(struct mm_struct *mm)
  {
- 	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm)))
  		__cpuc_flush_user_all();
  }
  
  static inline void
--- 232,242 ----
  
  static inline void vivt_flush_cache_mm(struct mm_struct *mm)
  {
+ 	if (fcse_mm_in_cache(mm)) {
+ 		unsigned seq = fcse_flush_all_start();
  		__cpuc_flush_user_all();
+ 		fcse_flush_all_done(seq, 1);
+ 	}
  }
  
  static inline void
***************
*** 217,225 ****
  {
  	struct mm_struct *mm = vma->vm_mm;
  
- 	if (!mm || cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm)))
- 		__cpuc_flush_user_range(start & PAGE_MASK, PAGE_ALIGN(end),
- 					vma->vm_flags);
  }
  
  static inline void
--- 244,254 ----
  {
  	struct mm_struct *mm = vma->vm_mm;
  
+ 	if (!mm || fcse_mm_in_cache(mm)) {
+ 		start = fcse_va_to_mva(mm, start & PAGE_MASK);
+ 		end = fcse_va_to_mva(mm, PAGE_ALIGN(end));
+ 		__cpuc_flush_user_range(start, end, vma->vm_flags);
+ 	}
  }
  
  static inline void
***************
*** 227,234 ****
  {
  	struct mm_struct *mm = vma->vm_mm;
  
- 	if (!mm || cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm))) {
- 		unsigned long addr = user_addr & PAGE_MASK;
  		__cpuc_flush_user_range(addr, addr + PAGE_SIZE, vma->vm_flags);
  	}
  }
--- 256,264 ----
  {
  	struct mm_struct *mm = vma->vm_mm;
  
+ 	if (!mm || fcse_mm_in_cache(mm)) {
+ 		unsigned long addr;
+ 		addr = fcse_va_to_mva(mm, user_addr) & PAGE_MASK;
  		__cpuc_flush_user_range(addr, addr + PAGE_SIZE, vma->vm_flags);
  	}
  }
***************
*** 253,266 ****
   * Harvard caches are synchronised for the user space address range.
   * This is used for the ARM private sys_cacheflush system call.
   */
- #define flush_cache_user_range(start,end) \
- 	__cpuc_coherent_user_range((start) & PAGE_MASK, PAGE_ALIGN(end))
  
  /*
   * Perform necessary cache operations to ensure that data previously
   * stored within this range of addresses can be executed by the CPU.
   */
- #define flush_icache_range(s,e)		__cpuc_coherent_kern_range(s,e)
  
  /*
   * Perform necessary cache operations to ensure that the TLB will
--- 283,304 ----
   * Harvard caches are synchronised for the user space address range.
   * This is used for the ARM private sys_cacheflush system call.
   */
+ #define flush_cache_user_range(start, end)				\
+ 	({								\
+ 		struct mm_struct *_mm = current->mm;			\
+ 		unsigned long _start, _end;				\
+ 		_start = fcse_va_to_mva(_mm, start) & PAGE_MASK;	\
+ 		_end = PAGE_ALIGN(fcse_va_to_mva(_mm, end));		\
+ 		__cpuc_coherent_user_range(_start, _end);		\
+ 	})
  
  /*
   * Perform necessary cache operations to ensure that data previously
   * stored within this range of addresses can be executed by the CPU.
   */
+ #define flush_icache_range(s,e)						\
+ 	__cpuc_coherent_kern_range(fcse_va_to_mva(current->mm, (s)),	\
+ 				   fcse_va_to_mva(current->mm, (e)))
  
  /*
   * Perform necessary cache operations to ensure that the TLB will
***************
*** 301,307 ****
  	extern void __flush_anon_page(struct vm_area_struct *vma,
  				struct page *, unsigned long);
  	if (PageAnon(page))
- 		__flush_anon_page(vma, page, vmaddr);
  }
  
  #define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
--- 339,346 ----
  	extern void __flush_anon_page(struct vm_area_struct *vma,
  				struct page *, unsigned long);
  	if (PageAnon(page))
+ 		__flush_anon_page(vma, page,
+ 				  fcse_va_to_mva(vma->vm_mm, vmaddr));
  }
  
  #define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
***************
*** 332,340 ****
   */
  static inline void flush_cache_vmap(unsigned long start, unsigned long end)
  {
- 	if (!cache_is_vipt_nonaliasing())
  		flush_cache_all();
- 	else
  		/*
  		 * set_pte_at() called from vmap_pte_range() does not
  		 * have a DSB after cleaning the cache line.
--- 371,381 ----
   */
  static inline void flush_cache_vmap(unsigned long start, unsigned long end)
  {
+ 	if (!cache_is_vipt_nonaliasing()) {
+ 		unsigned seq = fcse_flush_all_start();
  		flush_cache_all();
+ 		fcse_flush_all_done(seq, 1);
+ 	} else
  		/*
  		 * set_pte_at() called from vmap_pte_range() does not
  		 * have a DSB after cleaning the cache line.
***************
*** 344,351 ****
  
  static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
  {
- 	if (!cache_is_vipt_nonaliasing())
  		flush_cache_all();
  }
  
  #endif
--- 385,395 ----
  
  static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
  {
+ 	if (!cache_is_vipt_nonaliasing()) {
+ 		unsigned seq = fcse_flush_all_start();
  		flush_cache_all();
+ 		fcse_flush_all_done(seq, 1);
+ 	}
  }
  
  #endif
