***************
*** 2678,2684 ****
  
  	smp_wmb();
  	raw_spin_lock_irqsave(&p->pi_lock, flags);
- 	if (!(p->state & state))
  		goto out;
  
  	success = 1; /* we're going to change ->state */
--- 2678,2685 ----
  
  	smp_wmb();
  	raw_spin_lock_irqsave(&p->pi_lock, flags);
+ 	if (!(p->state & state) ||
+ 	    (p->state & (TASK_NOWAKEUP|TASK_ATOMICSWITCH)))
  		goto out;
  
  	success = 1; /* we're going to change ->state */
***************
*** 3131,3143 ****
  #endif
  	if (current->set_child_tid)
  		put_user(task_pid_vnr(current), current->set_child_tid);
  }
  
  /*
   * context_switch - switch to the new MM and the new
   * thread's register state.
   */
- static inline void
  context_switch(struct rq *rq, struct task_struct *prev,
  	       struct task_struct *next)
  {
--- 3132,3146 ----
  #endif
  	if (current->set_child_tid)
  		put_user(task_pid_vnr(current), current->set_child_tid);
+ 
+  	ipipe_init_notify(current);
  }
  
  /*
   * context_switch - switch to the new MM and the new
   * thread's register state.
   */
+ static inline int
  context_switch(struct rq *rq, struct task_struct *prev,
  	       struct task_struct *next)
  {
***************
*** 3179,3190 ****
  	switch_to(prev, next, prev);
  
  	barrier();
  	/*
  	 * this_rq must be evaluated again because prev may have moved
  	 * CPUs since it called schedule(), thus the 'rq' on its stack
  	 * frame will be invalid.
  	 */
  	finish_task_switch(this_rq(), prev);
  }
  
  /*
--- 3182,3204 ----
  	switch_to(prev, next, prev);
  
  	barrier();
+ 
+ #ifdef CONFIG_IPIPE_DELAYED_ATOMICSW
+ 	current->state &= ~TASK_ATOMICSWITCH;
+ #else
+ 	prev->state &= ~TASK_ATOMICSWITCH;
+ #endif
+ 	if (task_hijacked(prev))
+ 		return 1;
+ 
  	/*
  	 * this_rq must be evaluated again because prev may have moved
  	 * CPUs since it called schedule(), thus the 'rq' on its stack
  	 * frame will be invalid.
  	 */
  	finish_task_switch(this_rq(), prev);
+ 
+ 	return 0;
  }
  
  /*
***************
*** 4098,4103 ****
  
  void __kprobes add_preempt_count(int val)
  {
  #ifdef CONFIG_DEBUG_PREEMPT
  	/*
  	 * Underflow?
--- 4112,4118 ----
  
  void __kprobes add_preempt_count(int val)
  {
+  	ipipe_check_context(ipipe_root_domain);
  #ifdef CONFIG_DEBUG_PREEMPT
  	/*
  	 * Underflow?
***************
*** 4120,4125 ****
  
  void __kprobes sub_preempt_count(int val)
  {
  #ifdef CONFIG_DEBUG_PREEMPT
  	/*
  	 * Underflow?
--- 4135,4141 ----
  
  void __kprobes sub_preempt_count(int val)
  {
+  	ipipe_check_context(ipipe_root_domain);
  #ifdef CONFIG_DEBUG_PREEMPT
  	/*
  	 * Underflow?
***************
*** 4168,4173 ****
   */
  static inline void schedule_debug(struct task_struct *prev)
  {
  	/*
  	 * Test if we are atomic. Since do_exit() needs to call into
  	 * schedule() atomically, we ignore that path for now.
--- 4184,4190 ----
   */
  static inline void schedule_debug(struct task_struct *prev)
  {
+ 	ipipe_check_context(ipipe_root_domain);
  	/*
  	 * Test if we are atomic. Since do_exit() needs to call into
  	 * schedule() atomically, we ignore that path for now.
***************
*** 4219,4225 ****
  /*
   * __schedule() is the main scheduler function.
   */
- static void __sched __schedule(void)
  {
  	struct task_struct *prev, *next;
  	unsigned long *switch_count;
--- 4236,4242 ----
  /*
   * __schedule() is the main scheduler function.
   */
+ static int __sched __schedule(void)
  {
  	struct task_struct *prev, *next;
  	unsigned long *switch_count;
***************
*** 4233,4238 ****
  	rcu_note_context_switch(cpu);
  	prev = rq->curr;
  
  	schedule_debug(prev);
  
  	if (sched_feat(HRTICK))
--- 4250,4259 ----
  	rcu_note_context_switch(cpu);
  	prev = rq->curr;
  
+  	if (unlikely(prev->state & TASK_ATOMICSWITCH))
+ 		/* Pop one disable level -- one still remains. */
+ 		preempt_enable();
+ 
  	schedule_debug(prev);
  
  	if (sched_feat(HRTICK))
***************
*** 4279,4285 ****
  		rq->curr = next;
  		++*switch_count;
  
- 		context_switch(rq, prev, next); /* unlocks the rq */
  		/*
  		 * The context switch have flipped the stack from under us
  		 * and restored the local variables which were saved when
--- 4300,4307 ----
  		rq->curr = next;
  		++*switch_count;
  
+  		if (context_switch(rq, prev, next)) /* unlocks the rq */
+   			return 1; /* task hijacked by higher domain */
  		/*
  		 * The context switch have flipped the stack from under us
  		 * and restored the local variables which were saved when
***************
*** 4288,4301 ****
  		 */
  		cpu = smp_processor_id();
  		rq = cpu_rq(cpu);
- 	} else
  		raw_spin_unlock_irq(&rq->lock);
  
  	post_schedule(rq);
  
  	preempt_enable_no_resched();
  	if (need_resched())
  		goto need_resched;
  }
  
  static inline void sched_submit_work(struct task_struct *tsk)
--- 4310,4327 ----
  		 */
  		cpu = smp_processor_id();
  		rq = cpu_rq(cpu);
+ 	} else {
+   		prev->state &= ~TASK_ATOMICSWITCH;
  		raw_spin_unlock_irq(&rq->lock);
+ 	}
  
  	post_schedule(rq);
  
  	preempt_enable_no_resched();
  	if (need_resched())
  		goto need_resched;
+ 
+ 	return 0;
  }
  
  static inline void sched_submit_work(struct task_struct *tsk)
***************
*** 4310,4321 ****
  		blk_schedule_flush_plug(tsk);
  }
  
- asmlinkage void __sched schedule(void)
  {
  	struct task_struct *tsk = current;
  
  	sched_submit_work(tsk);
- 	__schedule();
  }
  EXPORT_SYMBOL(schedule);
  
--- 4336,4347 ----
  		blk_schedule_flush_plug(tsk);
  }
  
+ asmlinkage int __sched schedule(void)
  {
  	struct task_struct *tsk = current;
  
  	sched_submit_work(tsk);
+ 	return __schedule();
  }
  EXPORT_SYMBOL(schedule);
  
***************
*** 4390,4396 ****
  
  	do {
  		add_preempt_count_notrace(PREEMPT_ACTIVE);
- 		__schedule();
  		sub_preempt_count_notrace(PREEMPT_ACTIVE);
  
  		/*
--- 4416,4423 ----
  
  	do {
  		add_preempt_count_notrace(PREEMPT_ACTIVE);
+ 		if (__schedule())
+ 			return;
  		sub_preempt_count_notrace(PREEMPT_ACTIVE);
  
  		/*
***************
*** 5191,5196 ****
  	oldprio = p->prio;
  	prev_class = p->sched_class;
  	__setscheduler(rq, p, policy, param->sched_priority);
  
  	if (running)
  		p->sched_class->set_curr_task(rq);
--- 5218,5224 ----
  	oldprio = p->prio;
  	prev_class = p->sched_class;
  	__setscheduler(rq, p, policy, param->sched_priority);
+   	ipipe_setsched_notify(p);
  
  	if (running)
  		p->sched_class->set_curr_task(rq);
***************
*** 5915,5920 ****
  
  	/* Set the preempt count _outside_ the spinlocks! */
  	task_thread_info(idle)->preempt_count = 0;
  
  	/*
  	 * The idle tasks have their own, simple scheduling class:
--- 5943,5949 ----
  
  	/* Set the preempt count _outside_ the spinlocks! */
  	task_thread_info(idle)->preempt_count = 0;
+ 	ipipe_check_context(ipipe_root_domain);
  
  	/*
  	 * The idle tasks have their own, simple scheduling class:
***************
*** 9317,9319 ****
  };
  #endif	/* CONFIG_CGROUP_CPUACCT */
  
--- 9346,9405 ----
  };
  #endif	/* CONFIG_CGROUP_CPUACCT */
  
+ 
+ #ifdef CONFIG_IPIPE
+ 
+ int ipipe_setscheduler_root(struct task_struct *p, int policy, int prio)
+ {
+ 	const struct sched_class *prev_class;
+ 	int oldprio, on_rq, running;
+ 	unsigned long flags;
+ 	struct rq *rq;
+ 
+ 	rq = task_rq_lock(p, &flags);
+ 	on_rq = p->on_rq;
+ 	running = task_current(rq, p);
+ 	if (on_rq)
+ 		deactivate_task(rq, p, 0);
+ 	if (running)
+ 		p->sched_class->put_prev_task(rq, p);
+ 
+ 	p->sched_reset_on_fork = 0;
+ 
+ 	oldprio = p->prio;
+ 	prev_class = p->sched_class;
+ 	__setscheduler(rq, p, policy, prio);
+ 	ipipe_setsched_notify(p);
+ 
+ 	if (running)
+ 		p->sched_class->set_curr_task(rq);
+ 	if (on_rq)
+ 		activate_task(rq, p, 0);
+ 
+ 	check_class_changed(rq, p, prev_class, oldprio);
+ 	task_rq_unlock(rq, p, &flags);
+ 
+ 	rt_mutex_adjust_pi(p);
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(ipipe_setscheduler_root);
+ 
+ int ipipe_reenter_root(struct task_struct *prev, int policy, int prio)
+ {
+ 	struct rq *rq = this_rq();
+ 
+ 	finish_task_switch(rq, prev);
+ 
+ 	post_schedule(rq);
+ 
+ 	preempt_enable_no_resched();
+ 
+ 	if (current->policy != policy || current->rt_priority != prio)
+ 		return ipipe_setscheduler_root(current, policy, prio);
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(ipipe_reenter_root);
+ 
+ #endif /* CONFIG_IPIPE */
